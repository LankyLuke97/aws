<h1>Thoughts on Thomas Dohmke's Article, "<a href="https://ashtom.github.io/developers-reinvented">Developers, Reinvented</a>"</h1>
<p>One of my phone's more irritating habits, among a plentiful selection of such examples, is to constantly suggest articles it thinks I might click. It's not that the articles are always crap, although they often are; more and more often they smack of AI generation, or at least AI aided ideation. The issue, I think is more that it highlights my own inability to not click on the bloody things; the notification appears, and, like clockwork, I click it. Why not just swipe it? Probably because these machines are terrible for us, but that's not the point right now.  </p>
<p>One such article that did suck me in was Thomas Dohmke's "Developers, Reinvented". For those of you not in the know, which included me until this came up, TD (as I shall now refer to him, as otherwise I will <em>definitely</em> spell his surname in all manner of incorrect ways) is the patron saint of version control with a side hustle as a minor bishop for auto-complete. Put another way, he's the CEO of GitHub, with an 'About' section on LinkedIn that just says "Building GitHub Copilot for the sake of developer happiness. Working backwards through his listed experience, though not in agonising detail, he has been CEO of GitHub for almost four years, Chief Product Officer for four months before that, and a VP of Strategic Programs for three years before that. Roles prior to Github include three-and-a-half years as product manager at Microsoft in various incarnations (I had to look up the precise definition of Program Manager, and then extrapolate from that to understand what might constitute a Principal PM Manager, Principal Group PM Manager, and then Principal Director of PM, all of which he lists as separate roles within Microsoft).  </p>
<p>It's only once I get back ten years or so that I find a listing for a role that may have included some actual development work, which was as Co-Founder and CEO of the entity that created HockeyApp / Bit Stadium. I say <em>may</em> have included development, since it really depends on how the company was structured; perhaps he was a technical founder, perhaps not. Prior to this, there are certainly listings that are indisputably developer roles, but they are even earlier, almost twenty years ago now. We'll come back to that, though I'm sure it's fairly obvious where I'm going with that train of thought.  </p>
<h2>The introduction concerns</h2>
<p>It took me but to reach the second sentence of the article to have some serious doubts about where this was going. The article itself had been recommended, as will come as no surprise, with some click-baity claim about AI replacing developers within two years - so imagine my displeasure at the the sentence beginning: "In recent interviews we spoke with 22 developers that already use AI tools heavily in their workflow, ...".  </p>
<p>Right. I'll try to give the piece the benefit of the doubt here, since TD himself didn't (I presume) write the copy that was presented in my notifications, but how on earth can one comfortably make claims on the basis of a sample of twenty-two developers who all already use AI tools "heavily" in their workflow? It's not exactly an unbiased sample, is it? But, as I say, benefit of the doubt here.  </p>
<p>Separately of the rest of this, there is one image in the whole article, placed immediately under this introductory paragraph, inserted below for us all to laugh at. I find it highly telling of the rest of the article. The blocks do not line up with the rest of the perspective, the physics do not make sense, and all of it is done in a style stolen from a man whohas famously called AI-generated animation "an insult to life itself". In a great twist of irony, that quote is itself now presented to me by AI, rather than the original article. God help us all. <br />
<img alt="Image of child in Studio Ghibli style playing with blocks" src="images/developers_reinvented_1_reinvent_yourself.png" />  </p>
<h2>It's empty crap</h2>
<p>The article is just dull. Remember when you were at secondary school and had to write about something you didn't care about? There would be that little word count at the bottom of Word, stubbornly refusing to move up nearly as fast as you needed, and you would just flesh out sentences with...nothing. Verbiage for the sake of it, because the metric at the forefront of your mind was word count. I presume most people felt that at least once, though maybe it was just me. That's not really something to be judged, I think, except perhaps to interrogate the value of giving school children tight topic guides on what they must write about. But no-one made TD write this. He chose to. So, why does it feel like there's nothing interesting being said? I actually find it a little tricky to critique, but it's raised some ire in me, so there must be something to unpack in that at least. Maybe I'll come back to that thought and, for now, just pick out some choice sections that feel particularly rubbish. Let's start with, "For many, the initial encounter with AI tools was met with a healthy dose of skepticism. 'Pretty cool, but gimmicky' was a common refrain, which now developers interpret as high initial expectations clashing with the <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321">jagged frontier</a> (hyperlink is TD's) of AI's unpredictable capabilities."  </p>
<p>Is this true? I think the first section was true for many people, that feeling of all this being a pretty damn cool technology that didn't have massive practical applications - although, I do personally know a man who spent something like twelve hours a day exploring GPT 3 (or 3.5?) for three days straight when it first released and started this neverending hype train, and I think he felt differently. However, to claim that developers now attribute this to the clash between initial high expectations and AI's (I think implied) <em>previous</em> unpredictability is quite the stretch. I certainly didn't have high expectations when I first experimented with ChatGPT and I would know a jagged frontier if it bit my arse, but I still wasn't that blown away. Nor do I, today, feel all that different about it. I just think it wasn't as massive an advancement as a lot of people either need or would very much like it to be.  </p>
<p>He then follows up about this being the point where many write off AI tools as unhelpful, "but those who continue experimenting reach pivotal 'aha! moments' of both time savings and figuring out tool capabilities and how to match them to their work." Again, this is quite the claim to make, because the implication is that time is the missing ingredient. "Just push through," TD seems to say, "and your efforts will be rewarded as you unlock the personalised assistance of AI." But there are plenty of relatively high-profile developers who have both spent plenty of time experimenting with a variety of AI tools and remained unimpressed, just as there are examples in TD's favour. This tenuous anecdotal evidence he supplies here is pretty worthless. Instead, how about the study that shows <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">AI slowing down experienced open-source developers</a>. One item of particular note to me was that the developers in this study (of whom, I should note, there were only sixteen and those from a very select group, though the number of tasks they completed were much higher, so I'm not sure what can be said about the statistical bias) weren't AI-skeptics, as I clearly am. The average estimate, both before and after tasks, was that there would be around a 20% speed-up, whereas there was in-fact an almost equivalent slowdown. Perhaps these developers who have experienced their "aha!" moments after experimenting with AI are actually just <em>bad</em> at estimating their tasks, the same as the rest of us; maybe they're also worse off with AI in their lives, and just don't yet know it.  </p>
<p>As a side note on this point, I was going to at least skim read the 'jagged frontier' paper, but upon reading the abstract I realised I'd seen it before and that it was a study of consultants and their tasks, performed in conjuction with Boston Consulting Group. I won't go into my views of the big consulting groups now, but suffice it to say, I read no further.  </p>
<p>Ok, I'll go into it a little bit. I've met one and only one person from one of the big consulting groups who I respect the abilities of, an absolutely brilliant young woman - and even then, I have seen her advising in positions at times where, behind the scenes, I think the person she's advising would be better off just thinking for themselves - although, on reflection, maybe they're just as out of their depth but without the advantage of her astounding intelligence. For the rest, I've nothing positive to say about their work.  </p>
<p>With that divergence aside, let's continue. Another choice snippet is a quote from one of these AI-pilled developers: "Either you have to embrace the AI, or you get out of your career." Again, is this true? There are certainly instances where that might be the case. I saw another suggested article (damn you, Google) of a developer being told they <em>had</em> to use Cursor for their interview, that there was too much code to write otherwise. I suppose you could argue they would be out of that particular job, if nothing else, if they refused to embrace our Lord and saviour, Cursor. I similarly have a friend who works at a theoretically much more technologically advanced company than I do, who has experienced his skip-3 (which he then explained was his manager's manager's manager, I think) harassing people into using AI coding tools in their work as much as possible. He didn't want to, didn't find they were that helpful for his work given the size of the codebase, but repeatedly had to justify why he wasn't. He certainly hasn't lost his job over it.  </p>
<p>The next section of an article is a breakdown of a four-stage "phased evolution", from "AI Skeptic" all the way to 'AI strategist', which is "fuelled by daily trial and error". Putting aside the details of these stages, which I would find exhausting to examine exhaustively, I will simply state some thoughts, since that's all TD appears to have done - there is no substantial backing to any of the points.<br />
- He states that "If [developers] persevere, they shed their expectations for one-shot success. [...] embrace iterative prompting, and realise that when getting poor results it's better to start over". This to me sounds like the process of learning, but without the benefits of learning. You don't get it right first time - you iterate - if you are getting poor results, maybe you need a different starting point or approach. So, same slow process, <em>but without the underlying improvement of your own abilities</em>, in which case a) what a meaningless way to live and b) you will forever be stuck at this level of knowledge, and only become better at prompting the AI tools themselves, since this is where your 'haptic' feedback of sorts comes from. Why is this a good thing?<br />
- He uses the phrases "AI Collaborator", "Actively co-creating with AI", and "learn AI's 'thought process'". Some people have made the not unreasonable argument that we can't say AI's <em>aren't</em> thinking since almost no-one can even begin to accurately discuss what thinking is, what self-awareness is, and then they say something about Qualia. I do not personally view this crop of LLM's as thinking, but I will acknowledge that I should be able to understand what a Qualia is before I make that assessment, so I will refrain. However, I also bloody think he should refrain from the heavy implication of these LLM's being thinking, reasoning beings.<br />
- The final section starts "AI Strategist", and I think this sums it up for me. I'll break it down in a fresh paragraph.  </p>
<p>Looking back at his experience on LinkedIn, it has been somewhere between eighteen and, generously, ten years since TD has sat and really wrote code, since he was the engineer who truly understood the software that underlay whatever product he was working on. At one point, he was the manager of a group of other Product Managers, a role that, best I can tell, is itself managing Project Managers, <em>who themselves try to manage engineering managers</em> of some variety of another, <em>who then often primarily manage engineers</em>. At that level of removal from the thing that actually makes money, from the actual work that makes things keep running, perhaps you might want to make sure your role in this neverending chain of managers is seen as important, that you are a strategist who can, as he says in the article, "[...] focus on the delegation and verification of a task," as though this is the more difficult, more exciting task we should be aspiring to. He describes verification, in the context of AI agents specifically, as "[...] all about tearing down the agent's work [...]. These developers moved from writing code to architecting and verifying the implementation work that is carried out by AI agents." Maybe I'm reading too much into it, but, truly, this reads like a man justifying his own role; it feels hard to ignore the parallels. As a side-note, I would not want to work for someone who views part of their job as 'tearing down' implementation work.  </p>
<p>I'm going to skip the next paragraph about half of the developers thinking 90% of code will be generated by AI in five years and half them expect it with two years. Whilst this was the statistic (in a mangled form) that originally drew me in, I've moved on at this point. Who gives a shit what twenty-two developers who are heavy AI users think about when they will no longer have to code. There are too many ways to rebutt that, whether from a statistical or psychological point of view.  </p>
<p>TD's next point is to claim, without substantiation, that these developers are <strong>realistic optimists</strong>, hand-waving away the possibility that they might just be insane. One of the developers, in defence of this claim, is quoted as saying, "I think of myself as [a] mediocre engineer, and I feel this AI reset is giving me a chance to build skills that will bring me closer to excellence."  </p>
<p>Without rehashing my point about learning above, I suspect AI doesn't help you learn, at all really. Unless you were to use it in a very specific way to augment your learning, leaning on spaced repitition, interleaving, concretisation, dual-encoding, retrieval, and concepts around desirable difficulties, you are going to just experience the Google effect on steroids. I don't have a study to back that up, though there are some that would at least be adjacent to some good evidence, but it feels intuitive. Take from it what you will, but I think that 'mediocre developer' would benefit more from just...spending the same time getting better at their craft.  </p>
<h2>Future jobs</h2>
<p>I don't know how much value there is diving into TD's takes on the future of the job market. People are notoriously bad at predicting the future and I doubt he's any different in that regard. There's the same desperate hope masked as sage fortelling that "traditional coding roles" will evolve and shift to, shocker, "delegating and verifying". Is it just me that really, really feels sorry for people whose entire life revolves around something as devoid of meaning as that? Does it have more meaning than it feels to me? The role of project manager is an inglorious though, in large companies or for large endeavours, necessary one that has been twisted, somehow, often, into being the person in charge. I don't think we should be so ecstatic to be adding more members to their ranks.  </p>
<p>This section has an overarching point that I do think is worth addressing. He talks about how developers don't mention "time saved" as the core benefit of working with agents, but were instead talking about increasing ambition. We should therefore update how we talk about and measure success when using these tools. After the initial efficiency gains, focus should be on raising the ceiling of the work and outcomes we can accomplish, which (apparently) helps explain the "unintuitive at first" observation that many of the developers we interviewed were paying for top-tier subscriptions.</p>
<p>Much like the developer I quoted earlier, I consider myself a mediocre egineer in many regards and above average in some. Perhaps AI could, for me too, increase my ambitions. However, so could getting better! I acknowledge that I am indeed now just rehashing my point from earlier, but my God, all this time spent learning to 'prompt engineer' could instead be directed towards actually learning how to engineer. The fantastic thing about the latter is that there is no upper limit! You can, with clever and focused work, continuously improve over your life. Improving at prompt engineering has an upper bound of 'whatever the AI is capable today less whatever rate limiting your overlord decides on' - I know which path I'd rather pick.  </p>
<p>This also feels like a case of shifting goalposts. One of the previously extolled virtues of AI-assisted coding was the increased pace of development. Now that there are studies showing the opposite, it seems to be quietly slipping into the background. In <a href="https://www.theverge.com/24221978/github-thomas-dohmke-ai-copilot-microsoft-openai-open-source">this interview</a> from last year, TD says the words 'fast' or 'faster' about 10 times by my count in relation to developers moving faster, whether it's with writing code with Copilot or moving to new models. Speed has always been one of the main selling points - until, it wasn't.</p>
<p>Additionally, do these tools really increase the ceiling of what you can do? I have mostly found them to be useful at debugging stacktraces for middleware that I don't fully understand or converting pseudocode/Python into a language I'm not familiar with - even then, I make sure to interrogate myself as to whether I wouldn't just be better off struggling with it for a bit so as not to have to rely forever more on ChatGPT for this problem. On the occasions where I have, for the sake of it, tried to work with an LLM to solve a problem <em>where I have enough knowledge to critique its response</em>, it has always been lacklustre. It's given me nonsense multithreading, or overcomplicated patterns for the simple-ish problem constraints; one time, a colleague used it to generate some bash in a script to solve a problem that I had diagnosed and suggested a fix for but didn't have access to implement, and it was just wrong - but my colleague was about to ship it without even reading it over! He's one of the smart ones, and he didn't even give it a second look until I pointed out the glaring error.  </p>
<p>In case someone would like to raise a complaint that I have only used ChatGPT and I haven't tried Clauwn 83 in Windcursor 2.x.91 - I do not care, because a) people are shilling all of these products and b) I explicitly do not want to get involved in the ecosystem. Also, the last example in the previous paragraph was a fully-integrated LLM-in-editor scenario, and I don't care that <code>n=1</code>, because <code>n=essentially 1</code> in TD's article.  </p>
<p>I don't even know what to comment on the last point he makes, about the unintuitive-at-first observation that many of the developers they interviewed were paying for top-tier subscriptions. Maybe it's because you've selected a group who heavily use LLMs and are therefore more willing to cough up the requisite cash; maybe, with the various rate-limiting going on, LLMs are only feasible on the top-tier subscriptions. Whatever the actual reason, I don't think the, again unsubstantiated, claim that "When you move from thinking about reducing effort to expanding scope, only the most advanced agentic capabilities will do." is the most likely e##xplanation for this phenomenon.  </p>
<h2>Skill issues</h2>
<p>TD's next section is a discussion of the selection of skills he sees as taking centre stage, "If we accept that the developer role is transforming[.]" - big if. So, what are these skills?<br />
1. AI fluency
2. Delegation and agent orchestration
3. Human-AI collaboration
4. Fundamentals (?)
5. Verification and quality control
6. Product understanding
7. Architecture and systems design  </p>
<p>Once again, we see the results of somewhere between 10 and 20 years of managing rather than developing. Points 2, 5, 6, and 7 are all the things that TD <em>has</em> still been doing in that time, and I guess he would like to make sure everyone respects them. Point 1 and 3 are meaningless, in my opinion. If AI is moving as fast as the salesmen would have us believe, with AGI coming any time by 2026 (or is it 2027? 2030? Wait, wasn't it already supposed to be here? Aren't I already out of a job?), then what good is AI fluency with the current crop of tools? What good is Human-AI collaboration <em>with AI as it currently stands</em>, if all of that is changing so damn fast? Why haven't you automated my job and sent me out to the woods, Altman! It also seems telling, to me, that TD thinks, "If anything, agentic workflows need an amplified version of [various verification related tasks], with extended test coverage and thinking about quality and security more upstream." He comes across as a man who views developers working below him as needing constant oversight and rigorous controls, and this meshes neatly with his view of LLM-driven agents. Count me out of that environment.  </p>
<p>I disagree with all of those being the important skills, but it's the point about fundamentals that makes me so curious. Why this chink in the armour? Why the acknowledgement that you still need to really know your stuff, the acknowledgement that, "Despite AI's code generation capabilities, developers need a deep understanding of programming basics, algorithms, data structures, and overall software systems." Where does this fit in with the rest? How do you square this with the mediocre developer? Does he not have a sufficient grasp of the above to acutally be using AI properly? If so, why use him as an example? If he does, in fact, have a sufficient grasp, then does that not mean he is more than a mediocre developer? This leads me onto the last section of his article. Stay strong, my rant is almost done.  </p>
<h2>Gods above, I hope no university deans are reading this shite</h2>
<p>For all the umbrage I take over this piece of writing, the section titled '<strong>Implications for education</strong>' takes the cake. TD argues for the reinvention of computer science eductation, suggesting that, since "Students will rely on AI to write increasingly large portions of code[, t]eaching in a way that evaluates rote syntax or memorization of APIs is becoming obsolete. Foundational coding remains a critical skill, but now as a way to understand systems, debug AI-generated code, and express ideas clearly to both humans and machines. Instead of 'write this loop' we will need to shift to 'understand what this code does, and what would break if you changed it."  </p>
<p>TD, my man, what computer science education did you get that focused on 'write this loop' beyond one introductory exercise in an introductory class? For all my many and varied issues with my university, credit where credit is due, none of them are based around that. The argument that foundational coding remains a skill <em>solely</em> insofar as it's used to verify AI-generated code is laughable and, unsurprisingly, backed up by nothing.  </p>
<p>He goes on. Thanks to the supposed shift that means developers will no longer need to write code, the curriculum should "[...] include AI-assisted coding, but focus on how to collaborate with AI: prompting, reviewing, editing, and validating output. In short, teach AI fluency." I don't know about anyone else, but if my (hypothetical) child was paying over Â£9,000 a year in tuition (or whatever you pay these days in freedom-bucks), and they were being taught helplessness at the hands of AI, I'd tell them to get their bloody money back. This is a fever-dream! Why not teach them those foundational skills you claim are still so vital <em>and that you actually might pay a university for</em>?  </p>
<p>It continues in this vein: "<strong>The future belongs to developers who can model systems, anticipate edge cases, and translate ambiguity into structure--skills that AI can't automate.</strong>" (Emphasis his.) Now, this I think I agree with, on balance. For a business, on average, you probably want developers who can do more than just take the exact requirement given them by some stakeholder and translate that into the most efficient piece of code possible. However, it's not a blanket statement, and nor does it follow that AI is even the best means to this end. "We need to teach abstraction [...] not just as pre-coding steps, but as the new coding." What happened to 'premature abstraction is the root of all evil.'? We've learned this lesson already! There are benefits to abstraction, absolutely - I personally love not having to write a custom program per computer, and I am aware of the argument that people have made that "LLM's are just natural language compilers", but if anyone says to me that abstraction is the new coding without a deeply, too-obviously sarcastic voice, I'm going to prod them vigorously with a barge pole until they step away from the code base. Let locality of behaviour into your life! Take the time to understand and think about the code you're writing! It's not the most important thing for your business, I'm sure, but nor are its details unimportant, to be left to the whims of Cursor (other auto-complete tools are available).  </p>
<p>The piece trundles along to its vaporous conclusion, with some limp comments about guiding and ciritquing AI being more "future-proof", as though he has an idea of what the future holds, and some insane idea that "Managing agents to achieve outcomes may souund unfulfilling to many, although we argue that's what developers have been doing on a lower level of abstraction, managing their computers via programming languages to achieve outcomes." - truly, you can make a meaningless analogy for anything in this space, because how in the hell is that either correct or useful - before finishing with a tiresome remark about embracing AI and viewing it as a growth opportunity.</p>
<h2>Take this with you, TD</h2>
<p>You say I should view AI as a growth opportunity. Studies show it to be anything but, that it inhibits cognition during and after usage, that it slows us down.  </p>
<p>You say that you are building the "tools of tomorrow", to usher me through this reinvention of my craft in ways that are "intuitive, delightful, and cater to developers' curiosity, keeping them fulfilled and happy during the transition." I say that I have never once felt delighted, nor inspired, by these tools, nor that they have engaged my curiosity. Not once has an answer or some code from an LLM fulfilled me one tenth the amount that solving a problem myself, or learning something new, has. They do not make me happy.  </p>
<p>All of you are pissing away billions in VC money, using resources at obscene rates, whether it be rare earth minerals or power or water or land or just my goddamn mental energy, which let me assure you is draining at a prodigious rate the more this farce continues. That makes you delusional. Realistically.</p>